---
layout: workshop
year: 2025
date: April 2025
location: <a href="https://iclr.cc/Conferences/2025">ICLR 2025</a> in Singapore
openreview: https://openreview.net/group?id=ICLR.cc/2025/Workshop/Re-Align

cover_image:
  image: "/assets/images/locations/vienna.jpg"
  author: Brandon Lim
  licence_name: "CC BY-SA 2.0"
  licence_link: "https://creativecommons.org/licenses/by-sa/2.0/"

buttons:
  - text: OpenReview
    url: https://openreview.net/group?id=ICLR.cc/2025/Workshop/Re-Align

show_news: true
show_dates: true
show_cfp: true
show_schedule: false
show_reviewers: false
show_sponsors: false

important_dates:
  - event: Paper submission deadline
    date: Monday, February 3rd, 2025
  - event: Reviewing deadline
    date: Friday, February 21st, 2025
  - event: Author notification
    date: Monday, March 3rd, 2025
  - event: Camera-ready copy (CRC) deadline
    date: Sunday, April 20th, 2025

speakers:
  - name: Phil Isola
    affiliation: MIT
    website: https://web.mit.edu/phillipi/
    image: profile_isola.jpeg
  - name: Janet Wiles
    affiliation: University of Queensland
    website: https://eecs.uq.edu.au/profile/2444/janet-wiles
    image: profile_wiles.jpg
  - name: Alex Williams
    affiliation: NYU
    website: https://neurostatslab.org/
    image: profile_williams.jpg

organizers:
  - name: Brian Cheung
    affiliation: MIT
    website: https://briancheung.github.io/
    image: brian.jpg
  - name: Dota Dong
    affiliation: MPI for Psycholinguistics
    website: https://tianaidong.github.io/
    image: dota.jpg
  - name: Erin Grant
    affiliation: UCL
    website: https://eringrant.github.io/
    image: erin.jpeg
  - name: Ilia Sucholutsky
    affiliation: NYU
    website: https://ilia10000.github.io/
    image: ilia.jpeg
  - name: Lukas Muttenthaler
    affiliation: TU Berlin
    website: https://lukasmut.github.io/
    image: lukas.jpeg
  - name: Siddharth Suresh
    affiliation: University of Wisconsin-Madison
    website: https://www.sidsuresh.com/
    image: sid.jpg

news:
  - content: "The 2025 edition of Re-Align was accepted as a workshop at ICLR 2025! Stay tuned for the call for contributed papers."

about: |
  Both natural and artificial intelligences form representations of the world that they use to reason, make decisions,
  and communicate. Despite extensive research across machine learning, neuroscience, and cognitive science, it
  remains unclear what the most appropriate ways are to compare and align the representations of intelligent systems
  ([Sucholutsky et al., 2023](https://arxiv.org/abs/2310.13018)). 
  In the second edition of the Workshop on Representational Alignment (Re-Align), we
  bring together researchers from diverse fields who study representational alignment to make concrete progress on
  this set of open interdisciplinary problems. We invite researchers across the machine learning, neuroscience, and
  cognitive science communities to participate and contribute to the workshop in two main ways:

  **First, in the form of invited talks, contributed papers, and participation in structured discussions** that
  address questions of representational alignment and related questions in fields of machine learning interpretability
  and safety, which are all of ongoing interest at ICLR and other machine learning conferences. These questions
  stem from the following central theme: When and why do intelligence systems learn aligned representations, and
  how can scientists and engineers intervene on this alignment? *For example*, due to the increased use of large-scale
  models across various industries and scientific areas (e.g., [Gemini Team Google, 2023](https://arxiv.org/abs/2312.11805); 
  [OpenAI, 2023](https://arxiv.org/abs/2303.08774)), the field is
  in need of identifying ways to better interpret and ultimately understand these systems. Model interpretability is
  tightly linked to the representations formed by those systems 
  (see [Doshi-Velez and Kim, 2017](https://arxiv.org/abs/1702.08608); 
  [Sucholutsky et al., 2023](https://arxiv.org/abs/2310.13018); 
  [Lampinen et al., 2024](https://arxiv.org/abs/2405.05847); 
  [Muttenthaler et al., 2024](https://arxiv.org/abs/2409.06509)). 
  Thus, a better understanding of representations and
  their alignment to a reference system—usually, a human target—will in turn foster the models’ interpretability
  and explainability. 
  *Another set of questions* focuses on the connections between representation learning and
  computational neuroscience and cognitive science. These fields have relatively independently developed approaches
  for evaluating and increasing the alignment between artificial intelligence and human intelligence systems at neural
  and behavioral levels 
  ([Collins et al., 2024](https://arxiv.org/abs/2408.03943); 
  [Muttenthaler et al., 2024](https://arxiv.org/abs/2409.06509); 
  [Dorszewski et al., 2024](https://arxiv.org/abs/2409.06362); 
  [Bonnen et al., 2024](https://arxiv.org/abs/2409.05862);
  [Sundaram et al., 2024)](https://arxiv.org/abs/2410.10817). Our workshop enables an open discussion around identifying the most useful ways of
  measuring and increasing the alignment of artificial intelligence with human intelligence systems.

  **Second, by participating in our workshop hackathon**.
  Since the first iteration of Re-Align workshop, there have been numerous debates around the metrics that
  we use to measure representational similarity, which is often taken as a measure of representational alignment (e.g.,
  [Cloos et al., 2024](https://arxiv.org/abs/2407.07059); 
  [Khosla et al., 2024](https://doi.org/10.1101/2024.06.20.599957); 
  [Lampinen et al., 2024](https://arxiv.org/abs/2405.05847); 
  [Schaeffer et al., 2024](https://openreview.net/forum?id=vbtj05J68r)). 
  As of now, there is little consensus on which metric is best aligned(!) with the goal of identifying similarity
  between systems (see 
  [Sucholutsky et al., 2023](https://arxiv.org/abs/2310.13018); 
  [Harvey et al., 2024](https://proceedings.mlr.press/v243/harvey24a); 
  [Schaeffer et al., 2024](https://openreview.net/forum?id=vbtj05J68r)). We are confident that
  the hackathon component of the workshop will be helpful in articulating the consequences of these methodologies
  by facilitating a common language among researchers and as a result increase the reproducibility of research in this
  subdomain.
---
